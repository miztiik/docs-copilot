{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Docs Copilot â€“ A Generative AI App for Searching Documentation\n",
    "\n",
    "Github: https://github.com/miztiik/docs-copilot\n",
    "\n",
    "Google Colab: https://colab.research.google.com/github//miztiik/docs-copilot/blob/main/src/docs_copilot_w_chroma.ipynb\n",
    "\n",
    "![Miztiik Automation: Docs Copilot](../images/miztiik_automation_docs_copilot_using_llm_rag_01.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT NEXT LINE TO SKIP THIS CELL EXECUTION\n",
    "# %%script skipping --no-raise-error\n",
    "\n",
    "# Install the dependencies for the project\n",
    "\n",
    "%pip install --quiet pandas\n",
    "%pip install --quiet numpy\n",
    "%pip install --quiet openai\n",
    "%pip install --quiet python-dotenv\n",
    "%pip install --quiet tenacity\n",
    "%pip install --quiet tiktoken \n",
    "%pip install --quiet --upgrade chromadb \n",
    "%pip install --quiet langchain\n",
    "%pip install --quiet sentence-transformers\n",
    "\n",
    "# For progress bar and process time\n",
    "%pip install --quiet tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from dotenv import load_dotenv, dotenv_values\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# For Vector Embeddings store\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from chromadb.config import Settings\n",
    "\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "# For exponential backoff\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the environment variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the environment variables\n",
    "\n",
    "# specify the name of the .env file name\n",
    "env_path = \".env\"\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "# config = dotenv_values(env_name)\n",
    "\n",
    "# Raw Data Path\n",
    "RAW_DATA_PATH = \"./../data/raw/azure_docs/\"\n",
    "\n",
    "# DB_PATH = os.getenv(\"DB_PATH\")\n",
    "DB_PATH = \"./../data/processed/dbs/azure_docs/\"\n",
    "COLLECTION_NAME = \"fn_markdown\"\n",
    "\n",
    "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "AZURE_OPENAI_API_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "\n",
    "\n",
    "# Azure OpenAI Models\n",
    "embeddings_deployment_name = \"nice\"\n",
    "embeddings_deployment_model = \"text-embedding-ada-002\"\n",
    "completions_deployment_name = \"hellno\"\n",
    "completions_deployment_model = \"gpt-35-turbo-16k\"\n",
    "\n",
    "# Hugging Face Models\n",
    "hf_model_name = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "print(f\"AZURE_OPENAI_ENDPOINT: {os.getenv('AZURE_OPENAI_ENDPOINT')}\")\n",
    "print(f\"AZURE_OPENAI_API_VERSION: {os.getenv('AZURE_OPENAI_API_VERSION')}\")\n",
    "\n",
    "if AZURE_OPENAI_API_KEY is None:\n",
    "    print(\"Please set the AZURE_OPENAI_API_KEY environment variable\")\n",
    "    raise EnvironmentError(\"Please set the AZURE_OPENAI_API_KEY environment variable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logging.info(\"Welcome to Miztiik Automation for Docs Copilot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Hugging Face Embeddings & Test them\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup Models Cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the Hugging Face Model Cache\n",
    "MODEL_CACHE_DIR = \"./../models_cache/hf/\"\n",
    "\n",
    "# https://stackoverflow.com/questions/63312859/how-to-change-huggingface-transformers-default-cache-directory\n",
    "os.environ[\"HF_HOME\"] = MODEL_CACHE_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_txt = [\"This is an Miztikal World\", \"Lets rejoice to together\"]\n",
    "\n",
    "# Inititalise the embedding\n",
    "embeddings_fn_by_hf = HuggingFaceEmbeddings()\n",
    "\n",
    "print(f\"Refer Embedding Leaderboard: https://huggingface.co/spaces/mteb/leaderboard\")\n",
    "print(\n",
    "    f\"Refer Embedding Leaderboard: https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "hf_model = SentenceTransformer(\n",
    "    \"sentence-transformers/all-MiniLM-L6-v2\", cache_folder=MODEL_CACHE_DIR\n",
    ")\n",
    "sample_txt_embeddings = hf_model.encode(sample_txt)\n",
    "\n",
    "if sample_txt_embeddings is None:\n",
    "    print(\"Unable to embedd the query\")\n",
    "\n",
    "embeddings_fn_by_hf = HuggingFaceEmbeddings(\n",
    "    model_name=hf_model_name,\n",
    ")\n",
    "\n",
    "# Inititalise the embedding fn for Chroma Document Level Embedding\n",
    "embeddings_fn_4_collections = SentenceTransformerEmbeddings(\n",
    "    model_name=\"all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "\n",
    "sample_txt_embeddings = embeddings_fn_by_hf.embed_query(sample_txt[0])\n",
    "\n",
    "if sample_txt_embeddings is None:\n",
    "    print(f\"Unable to embedd the query\")\n",
    "else:\n",
    "    print(f\"Successfully generated embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Vector Database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_chromadb(db_path):\n",
    "    if os.path.exists(db_path) and os.path.isdir(db_path):\n",
    "        print(\"Deleting existing ChromaDB at\", db_path)\n",
    "        shutil.rmtree(db_path)\n",
    "\n",
    "\n",
    "def write_to_vec_store_collection(\n",
    "    db_path, collection_name, docs_list, ids_list, embeddings_list, metadatas_list\n",
    "):\n",
    "    docs_vs_status = False\n",
    "    # from chromadb.config import Settings\n",
    "    # client = chromadb.Client(path=db_path, Settings=Settings(anonymized_telemetry=False))\n",
    "    try:\n",
    "        vs_client = chromadb.PersistentClient(\n",
    "            path=db_path,\n",
    "        )\n",
    "        vs_collection = vs_client.get_or_create_collection(name=collection_name)\n",
    "\n",
    "        vs_collection.add(\n",
    "            documents=docs_list,\n",
    "            ids=ids_list,\n",
    "            embeddings=embeddings_list,\n",
    "            metadatas=metadatas_list,\n",
    "        )\n",
    "\n",
    "        vs_client = None\n",
    "        docs_vs_status = True\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {str(e)}\")\n",
    "        raise e\n",
    "    return docs_vs_status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest Documents to ChromaDB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Tokens in each document\n",
    "def count_tokens(model_name, docs):\n",
    "    token_count = 0\n",
    "    tokenizer = tiktoken.encoding_for_model(model_name)\n",
    "    token_count = [len(tokenizer.encode(d.page_content)) for d in docs]\n",
    "    print(token_count)\n",
    "    return token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "    md_headers_to_split_on, strip_headers=False\n",
    ")\n",
    "\n",
    "# Initialize text splitter and embeddings\n",
    "chunk_size = 1000\n",
    "chunk_overlap = 50\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    # length_function=len,\n",
    "    separators=[\"#\", \"##\", \"###\", \"\\n\\n\", \"\\n\", \"(?<=\\, )\"],\n",
    ")\n",
    "\n",
    "\n",
    "def get_files_with_extension(dir_path, doc_extension):\n",
    "    \"\"\"\n",
    "    Get a list of files in a directory (including subdirectories) matching a file extension.\n",
    "    \"\"\"\n",
    "    files = []\n",
    "\n",
    "    for dirpath, dirnames, filenames in os.walk(dir_path):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith(doc_extension):\n",
    "                files.append(os.path.join(dirpath, filename))\n",
    "    return files\n",
    "\n",
    "\n",
    "def ingest_docs_in_dir_to_chromadb(docs_path, doc_extension):\n",
    "    # Process each file in the docs_path directory\n",
    "    for file in os.listdir(docs_path):\n",
    "        if file.endswith(doc_extension):\n",
    "            file_path = os.path.join(docs_path, file)\n",
    "\n",
    "            print(f\"Processing {file_path} file.\")\n",
    "\n",
    "            with open(file_path) as f:\n",
    "                try:\n",
    "                    documents_list = []\n",
    "                    ids_list = []\n",
    "                    metadatas_list = []\n",
    "                    embeddings_list = []\n",
    "\n",
    "                    file_contents = f.read()\n",
    "\n",
    "                    file_chunks = text_splitter.split_text(file_contents)\n",
    "\n",
    "                    for i, file_chunk in enumerate(file_chunks):\n",
    "                        documents_list.append(file_chunk)\n",
    "                        ids_list.append(f\"{file}_{i}\")\n",
    "                        metadatas_list.append({\"source\": file, \"chunk_id\": i})\n",
    "                        # INGEST TO VECTOR STORE\n",
    "                        doc_vectors = embeddings_fn_by_hf.embed_query(file_chunk)\n",
    "                        embeddings_list.append(doc_vectors)\n",
    "\n",
    "                    # Ingest the documents into the vector store\n",
    "                    __vs_resp = write_to_vec_store_collection(\n",
    "                        DB_PATH,\n",
    "                        COLLECTION_NAME,\n",
    "                        documents_list,\n",
    "                        ids_list,\n",
    "                        embeddings_list,\n",
    "                        metadatas_list,\n",
    "                    )\n",
    "\n",
    "                    if not __vs_resp:\n",
    "                        raise Exception(f\"Error occurred while processing {file} file.\")\n",
    "\n",
    "                    print(f\"file: {file} added to vector store.\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error occurred while processing {file} file.\")\n",
    "                    print(str(e))\n",
    "                    raise e\n",
    "\n",
    "    print(f\"{len(os.listdir(docs_path))} files added to vector store\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the documents into chunks\n",
    "\n",
    "![Miztiik Automation: Docs Copilot](../images/miztiik_automation_docs_copilot_using_llm_rag_02.png)\n",
    "\n",
    "### Store the chunks in ChromaDB\n",
    "\n",
    "![Miztiik Automation: Docs Copilot](../images/miztiik_automation_docs_copilot_using_llm_rag_03.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest_docs_in_dir_to_chromadb(RAW_DATA_PATH, \".md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Database from disk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_chroma_client = chromadb.PersistentClient(path=DB_PATH)\n",
    "docs_collection = vs_chroma_client.get_or_create_collection(\n",
    "    name=COLLECTION_NAME)\n",
    "\n",
    "print(f\"ChromaDB Heartbeat: {vs_chroma_client.heartbeat()}\")\n",
    "print(f\"ChromaDB Collections: {vs_chroma_client.list_collections()}\")\n",
    "\n",
    "\n",
    "# Verify ChromaDB is setup correctly, by checking document count\n",
    "print(f\"ChromaDB has {docs_collection.count()} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm that the data was inserted by looking at the database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs_collection.peek(2)\n",
    "pd.DataFrame(docs_collection.peek(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query the database\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query for text matching the query string\n",
    "\n",
    "![Miztiik Automation: Docs Copilot](../images/miztiik_automation_docs_copilot_using_llm_rag_04.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_txt(query, docs_collection):\n",
    "    passage = docs_collection.query(query_texts=[query], n_results=1)[\n",
    "        \"documents\"][0][0]\n",
    "    return passage\n",
    "\n",
    "\n",
    "def get_relevant_docs(query, docs_collection):\n",
    "    docs = docs_collection.query(\n",
    "        query_texts=[query], n_results=5, include=[\"documents\"]\n",
    "    )\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform embedding search\n",
    "usr_query_1 = \"How to configure Azure Functions with a virtual network\"\n",
    "usr_query = \"What are Azure Functions\"\n",
    "\n",
    "matching_txt = get_relevant_txt(usr_query, docs_collection)\n",
    "\n",
    "print(f\"Matching Text: {matching_txt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_docs = get_relevant_docs(usr_query, docs_collection)\n",
    "\n",
    "print(f\"Total docs found: {len(matching_docs['documents'][0])}\")\n",
    "\n",
    "print(\"\\033[31m\" + \"User Input:\" + usr_query + \"\\033[0m\")\n",
    "\n",
    "\n",
    "for result in matching_docs[\"documents\"]:\n",
    "    for i in result:\n",
    "        print(i)\n",
    "        print(\"\\033[32m\" + \"+++++++++++++++++++++++++++++++++++\" + \"\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a prompt to pass to GPT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup Azure OpenAI Client & Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    ")\n",
    "\n",
    "\n",
    "def embeddings_generator_az_oai(text, model=\"nice\"):\n",
    "    # model = \"deployment_name\"\n",
    "    return client.embeddings.create(input=[text], model=model).data[0].embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify if Azure OpenAI Embeddings are generated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_txt_embeddings = embeddings_generator_az_oai(\n",
    "    \"Welcome to Miztiikal World\",\n",
    "    # model should be set to the deployment name you chose when you deployed the text-embedding-ada-002 (Version 2) model\n",
    "    model=embeddings_deployment_name,\n",
    ")\n",
    "\n",
    "if sample_txt_embeddings is None:\n",
    "    print(\"No embeddings found\")\n",
    "else:\n",
    "    print(f\"Successfully generated embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function helps to ground the model with prompts and system instructions.\n",
    "\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(10))\n",
    "def generate_completion(usr_query, r_data, num_tokens=2000):\n",
    "    system_prompt = \"\"\"You are an intelligent assistant for Microsoft Azure services.\n",
    "    Use the following pieces of context to answer the question at the end. Question is enclosed in <question></question>.\n",
    "    Do keep the following things in mind when answering the question:\n",
    "        - If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "        - Keep the answer as concise as possible.\n",
    "        - Use only the context to answer the question. Context is enclosed in <context></context>\n",
    "        - The context contains one or more paragraph of text that is formatted as markdown. When answering, remove the sentences from the markdown that contain markdown links.\n",
    "        - If the answer is not found in context, simply output \"I'm sorry but I do not know the answer to your question. Please visit Microsoft Learn (https://learn.microsoft.com) or ask a question on StackOverflow (https://stackoverflow.com/questions/tagged/azure).\n",
    "        - Do not include the code in output unless the question is asked to produce the code.\n",
    "        \"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": usr_query},\n",
    "        {\"role\": \"assistant\", \"content\": r_data},\n",
    "    ]\n",
    "\n",
    "    resp = client.chat.completions.create(\n",
    "        model=completions_deployment_name,\n",
    "        messages=messages,\n",
    "        # max_tokens=num_tokens,\n",
    "        temperature=0,\n",
    "        stop=\"+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\\n\",\n",
    "    )\n",
    "\n",
    "    return resp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_docs = get_relevant_docs(usr_query, docs_collection)\n",
    "\n",
    "matching_docs_str = \"\".join(matching_docs[\"documents\"][0])\n",
    "\n",
    "resp = generate_completion(usr_query, matching_docs_str)\n",
    "\n",
    "print(f\"User_Query: \\033[32m {usr_query} \\033[0m\")\n",
    "print(f\"Response: \\033[36m { resp.choices[0].message.content} \\033[0m\")\n",
    "print(f\"total_tokens: \\033[36m { resp.usage.total_tokens} \\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usr_query = input(\"Prompt: \")\n",
    "\n",
    "\n",
    "while usr_query.lower() not in [\"end\", \"quit\", \"exit\", \"stop\"]:\n",
    "    if usr_query.lower() == \"stop\":\n",
    "        break\n",
    "\n",
    "    matching_docs = get_relevant_docs(usr_query, docs_collection)\n",
    "\n",
    "    print(f\"Total docs found: {len(matching_docs['documents'][0])}\")\n",
    "\n",
    "    matching_docs_str = \"\".join(matching_docs[\"documents\"][0])\n",
    "\n",
    "    # print(\"\\033[32m----------------------------------------------\\033[0m\")\n",
    "    # print(f\"{matching_docs}\")\n",
    "    # print(\"\\033[32m----------------------------------------------\\033[0m\")\n",
    "\n",
    "    resp = generate_completion(usr_query, matching_docs_str)\n",
    "\n",
    "    assistant_response = resp.choices[0].message.content\n",
    "\n",
    "    print(f\"total_tokens: \\033[36m { resp.usage.total_tokens} \\033[0m\")\n",
    "    print(f\"User_Query: \\033[32m {usr_query} \\033[0m\")\n",
    "    print(f\"Assistant: \\033[36m { assistant_response} \\033[0m\")\n",
    "\n",
    "    print(\n",
    "        \"\\033[32m\" + \"How can I help you? - Type 'stop' when you are done.\" + \"\\033[0m\"\n",
    "    )\n",
    "\n",
    "    usr_query = input(\"Question: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up the database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT NEXT LINE TO SKIP THIS CELL EXECUTION\n",
    "# %%script skipping --no-raise-error\n",
    "\n",
    "\n",
    "# To cleanup, you can delete the collection\n",
    "\n",
    "vs_chroma_client.delete_collection()\n",
    "\n",
    "vs_chroma_client.persist()\n",
    "\n",
    "\n",
    "# Or just nuke the persist directory\n",
    "\n",
    "# rm -rf data/processed/dbs/azure_docs/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
