{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Docs Copilot â€“ A Generative AI App for Searching Documentation\n",
    "\n",
    "Github: https://github.com/miztiik/docs-copilot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT NEXT LINE TO SKIP THIS CELL EXECUTION\n",
    "# %%script skipping --no-raise-error\n",
    "\n",
    "# Install the dependencies for the project\n",
    "\n",
    "%pip install --quiet numpy\n",
    "%pip install --quiet openai\n",
    "%pip install --quiet python-dotenv\n",
    "%pip install --quiet tenacity\n",
    "%pip install --quiet tiktoken \n",
    "%pip install --quiet --upgrade chromadb \n",
    "%pip install --quiet langchain\n",
    "\n",
    "# For progress bar and process time\n",
    "%pip install --quiet tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from dotenv import load_dotenv, dotenv_values\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# For Vector Embeddings store\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from chromadb.config import Settings\n",
    "\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "# For exponential backoff\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the environment variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AZURE_OPENAI_ENDPOINT: https://eastus.api.cognitive.microsoft.com/\n",
      "AZURE_OPENAI_API_VERSION: 2023-05-15\n"
     ]
    }
   ],
   "source": [
    "# Load the environment variables\n",
    "\n",
    "# specify the name of the .env file name\n",
    "# env_name = \"./env/docs_copilot.env\"\n",
    "# config = dotenv_values(env_name)\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Raw Data Path\n",
    "RAW_DATA_PATH = \"./../data/raw/azure_docs/\"\n",
    "\n",
    "# DB_PATH = os.getenv(\"DB_PATH\")\n",
    "DB_PATH = \"./../data/processed/dbs/azure_docs/\"\n",
    "COLLECTION_NAME = \"fn_markdown\"\n",
    "\n",
    "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "AZURE_OPENAI_API_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "\n",
    "\n",
    "# Azure OpenAI Models\n",
    "embeddings_deployment_name = \"nice\"\n",
    "embeddings_deployment_model = \"text-embedding-ada-002\"\n",
    "completions_deployment_name = \"hellno\"\n",
    "completions_deployment_model = \"gpt-35-turbo-16k\"\n",
    "\n",
    "# Hugging Face Models\n",
    "hf_model_name = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "print(f\"AZURE_OPENAI_ENDPOINT: {os.getenv('AZURE_OPENAI_ENDPOINT')}\")\n",
    "print(f\"AZURE_OPENAI_API_VERSION: {os.getenv('AZURE_OPENAI_API_VERSION')}\")\n",
    "\n",
    "if AZURE_OPENAI_API_KEY is None:\n",
    "    print(\"Please set the AZURE_OPENAI_API_KEY environment variable\")\n",
    "    raise EnvironmentError(\n",
    "        \"Please set the AZURE_OPENAI_API_KEY environment variable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logging.info(\"Welcome to Miztiik Automation for Docs Copilot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Hugging Face Embeddings & Test them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_txt = [\"This is an Miztikal World\", \"Lets rejoice to together\"]\n",
    "\n",
    "# Inititalise the embedding\n",
    "embeddings_fn_by_hf = HuggingFaceEmbeddings()\n",
    "\n",
    "print(f\"Refer Embedding Leaderboard: https://huggingface.co/spaces/mteb/leaderboard\")\n",
    "print(\n",
    "    f\"Refer Embedding Leaderboard: https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "hf_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "sample_txt_embeddings = hf_model.encode(sample_txt)\n",
    "\n",
    "if sample_txt_embeddings is None:\n",
    "    print(\"Unable to embedd the query\")\n",
    "\n",
    "embeddings_fn_by_hf = HuggingFaceEmbeddings(model_name=hf_model_name)\n",
    "\n",
    "# Inititalise the embedding fn for Chroma Document Level Embedding\n",
    "embeddings_fn_4_collections = SentenceTransformerEmbeddings(\n",
    "    model_name=\"all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "\n",
    "sample_txt_embeddings = embeddings_fn_by_hf.embed_query(sample_txt[0])\n",
    "\n",
    "if sample_txt_embeddings is None:\n",
    "    print(f\"Unable to embedd the query\")\n",
    "else:\n",
    "    print(f\"Successfully generated embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Vector Database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_chromadb(db_path):\n",
    "    if os.path.exists(db_path) and os.path.isdir(db_path):\n",
    "        print(\"Deleting existing ChromaDB at\", db_path)\n",
    "        shutil.rmtree(db_path)\n",
    "\n",
    "\n",
    "def write_to_vec_store_collection(\n",
    "    db_path, collection_name, docs_list, ids_list, embeddings_list, metadatas_list\n",
    "):\n",
    "    docs_vs_status = False\n",
    "    try:\n",
    "        vs_client = chromadb.PersistentClient(\n",
    "            path=db_path,\n",
    "        )\n",
    "        vs_collection = vs_client.get_or_create_collection(\n",
    "            name=collection_name)\n",
    "\n",
    "        vs_collection.add(\n",
    "            documents=docs_list,\n",
    "            ids=ids_list,\n",
    "            embeddings=embeddings_list,\n",
    "            metadatas=metadatas_list,\n",
    "        )\n",
    "\n",
    "        vs_client = None\n",
    "        docs_vs_status = True\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {str(e)}\")\n",
    "        raise e\n",
    "    return docs_vs_status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest Documents to ChromaDB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Tokens in each document\n",
    "def count_tokens(model_name, docs):\n",
    "    token_count = 0\n",
    "    tokenizer = tiktoken.encoding_for_model(model_name)\n",
    "    token_count = [len(tokenizer.encode(d.page_content)) for d in docs]\n",
    "    print(token_count)\n",
    "    return token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "    md_headers_to_split_on, strip_headers=False\n",
    ")\n",
    "\n",
    "chunk_size = 250\n",
    "chunk_overlap = 30\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    # length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"(?<=\\, )\", \" \", \"\", \"#\", \"##\", \"###\"],\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize text splitter and embeddings\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "\n",
    "\n",
    "def get_files_with_extension(dir_path, doc_extension):\n",
    "    \"\"\"\n",
    "    Get a list of files in a directory (including subdirectories) matching a file extension.\n",
    "    \"\"\"\n",
    "    files = []\n",
    "\n",
    "    for dirpath, dirnames, filenames in os.walk(dir_path):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith(doc_extension):\n",
    "                files.append(os.path.join(dirpath, filename))\n",
    "    return files\n",
    "\n",
    "\n",
    "def ingest_docs_in_dir_to_chromadb(docs_path, doc_extension):\n",
    "    # Process each file in the docs_path directory\n",
    "    for file in os.listdir(docs_path):\n",
    "        if file.endswith(doc_extension):\n",
    "            file_path = os.path.join(docs_path, file)\n",
    "\n",
    "            print(f\"Processing {file_path} file.\")\n",
    "\n",
    "            with open(file_path) as f:\n",
    "                try:\n",
    "                    documents_list = []\n",
    "                    ids_list = []\n",
    "                    metadatas_list = []\n",
    "                    embeddings_list = []\n",
    "\n",
    "                    file_contents = f.read()\n",
    "\n",
    "                    file_chunks = text_splitter.split_text(file_contents)\n",
    "\n",
    "                    for i, file_chunk in enumerate(file_chunks):\n",
    "                        documents_list.append(file_chunk)\n",
    "                        ids_list.append(f\"{file}_{i}\")\n",
    "                        metadatas_list.append({\"source\": file, \"chunk_id\": i})\n",
    "                        # INGEST TO VECTOR STORE\n",
    "                        doc_vectors = embeddings_fn_by_hf.embed_query(file_chunk)\n",
    "                        embeddings_list.append(doc_vectors)\n",
    "\n",
    "                    # Ingest the documents into the vector store\n",
    "                    __vs_resp = write_to_vec_store_collection(\n",
    "                        DB_PATH,\n",
    "                        COLLECTION_NAME,\n",
    "                        documents_list,\n",
    "                        ids_list,\n",
    "                        embeddings_list,\n",
    "                        metadatas_list,\n",
    "                    )\n",
    "\n",
    "                    if not __vs_resp:\n",
    "                        raise Exception(f\"Error occurred while processing {file} file.\")\n",
    "\n",
    "                    print(f\"file: {file} added to vector store.\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error occurred while processing {file} file.\")\n",
    "                    print(str(e))\n",
    "                    raise e\n",
    "\n",
    "    print(f\"{len(os.listdir(docs_path))} files added to vector store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest_docs_in_dir_to_chromadb(RAW_DATA_PATH, \".md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Database from disk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChromaDB Heartbeat: 1705862581158950700\n",
      "ChromaDB Collections: [Collection(name=fn_markdown)]\n",
      "ChromaDB has 4585 documents\n"
     ]
    }
   ],
   "source": [
    "vs_chroma_client = chromadb.PersistentClient(path=DB_PATH)\n",
    "docs_collection = vs_chroma_client.get_or_create_collection(name=COLLECTION_NAME)\n",
    "\n",
    "print(f\"ChromaDB Heartbeat: {vs_chroma_client.heartbeat()}\")\n",
    "print(f\"ChromaDB Collections: {vs_chroma_client.list_collections()}\")\n",
    "\n",
    "\n",
    "# Verify ChromaDB is setup correctly, by checking document count\n",
    "print(f\"ChromaDB has {docs_collection.count()} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm that the data was inserted by looking at the database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ids</th>\n",
       "      <th>embeddings</th>\n",
       "      <th>metadatas</th>\n",
       "      <th>documents</th>\n",
       "      <th>uris</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>add-bindings-existing-function.md_0</td>\n",
       "      <td>[-0.04316641017794609, -0.11902757734060287, -...</td>\n",
       "      <td>{'chunk_id': 0, 'source': 'add-bindings-existi...</td>\n",
       "      <td>---\\ntitle: Connect functions to other Azure s...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>add-bindings-existing-function.md_1</td>\n",
       "      <td>[-0.008143081329762936, -0.0859667956829071, -...</td>\n",
       "      <td>{'chunk_id': 1, 'source': 'add-bindings-existi...</td>\n",
       "      <td>## Local development       \\n\\nWhen you develo...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>add-bindings-existing-function.md_2</td>\n",
       "      <td>[-0.03155401349067688, -0.048116981983184814, ...</td>\n",
       "      <td>{'chunk_id': 2, 'source': 'add-bindings-existi...</td>\n",
       "      <td>### Manually add bindings based on examples\\n\\...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   ids  \\\n",
       "0  add-bindings-existing-function.md_0   \n",
       "1  add-bindings-existing-function.md_1   \n",
       "2  add-bindings-existing-function.md_2   \n",
       "\n",
       "                                          embeddings  \\\n",
       "0  [-0.04316641017794609, -0.11902757734060287, -...   \n",
       "1  [-0.008143081329762936, -0.0859667956829071, -...   \n",
       "2  [-0.03155401349067688, -0.048116981983184814, ...   \n",
       "\n",
       "                                           metadatas  \\\n",
       "0  {'chunk_id': 0, 'source': 'add-bindings-existi...   \n",
       "1  {'chunk_id': 1, 'source': 'add-bindings-existi...   \n",
       "2  {'chunk_id': 2, 'source': 'add-bindings-existi...   \n",
       "\n",
       "                                           documents  uris  data  \n",
       "0  ---\\ntitle: Connect functions to other Azure s...  None  None  \n",
       "1  ## Local development       \\n\\nWhen you develo...  None  None  \n",
       "2  ### Manually add bindings based on examples\\n\\...  None  None  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# docs_collection.peek(2)\n",
    "pd.DataFrame(docs_collection.peek(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query the database\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query for text matching the query string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_txt(query, docs_collection):\n",
    "    passage = docs_collection.query(query_texts=[query], n_results=1)[\"documents\"][0][0]\n",
    "    return passage\n",
    "\n",
    "\n",
    "def get_relevant_docs(query, docs_collection):\n",
    "    docs = docs_collection.query(\n",
    "        query_texts=[query], n_results=5, include=[\"documents\"]\n",
    "    )\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m---\n",
      "title: Guidance for developing Azure Functions\n",
      "description: Learn the Azure Functions concepts and techniques that you need to develop functions in Azure, across all programming languages and bindings.\n",
      "ms.assetid: d8efe41a-bef8-4167-ba97-f3e016fcd39e\n",
      "ms.topic: conceptual\n",
      "ms.date: 09/06/2023\n",
      "ms.custom: ignite-2022, devx-track-extended-java, devx-track-js, devx-track-python\n",
      "zone_pivot_groups: programming-languages-set-functions\n",
      "---\n",
      "\n",
      "# Azure Functions developer guide\n",
      "\n",
      "In Azure Functions, all functions share some core technical concepts and components, regardless of your preferred language or development environment. This article is language-specific. Choose your preferred language at the top of the article.\n",
      "\n",
      "This article assumes that you've already read the [Azure Functions overview](functions-overview.md).\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Perform embedding search\n",
    "usr_query_1 = \"How to configure Azure Functions with a virtual network\"\n",
    "usr_query = \"What are Azure Functions\"\n",
    "\n",
    "matching_txt = get_relevant_txt(usr_query, docs_collection)\n",
    "\n",
    "print(\"\\033[32m\" + matching_txt + \"\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_docs = get_relevant_docs(usr_query, docs_collection)\n",
    "\n",
    "print(f\"Total docs found: {len(matching_docs['documents'][0])}\")\n",
    "\n",
    "print(\"\\033[31m\" + \"User Input:\" + usr_query + \"\\033[0m\")\n",
    "\n",
    "\n",
    "for result in matching_docs[\"documents\"]:\n",
    "    for i in result:\n",
    "        print(i)\n",
    "        print(\"\\033[32m\" + \"+++++++++++++++++++++++++++++++++++\" + \"\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a prompt to pass to GPT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup Azure OpenAI Client & Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    ")\n",
    "\n",
    "\n",
    "def embeddings_generator_az_oai(text, model=\"nice\"):\n",
    "    # model = \"deployment_name\"\n",
    "    return client.embeddings.create(input=[text], model=model).data[0].embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify if Azure OpenAI Embeddings are generated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_txt_embeddings = embeddings_generator_az_oai(\n",
    "    \"Welcome to Miztiikal World\",\n",
    "    # model should be set to the deployment name you chose when you deployed the text-embedding-ada-002 (Version 2) model\n",
    "    model=embeddings_deployment_name,\n",
    ")\n",
    "\n",
    "if sample_txt_embeddings is None:\n",
    "    print(\"No embeddings found\")\n",
    "else:\n",
    "    print(f\"Successfully generated embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function helps to ground the model with prompts and system instructions.\n",
    "@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(10))\n",
    "def generate_completion(usr_query, r_data, num_tokens=2000):\n",
    "    system_prompt = \"\"\"You are an intelligent assistant for Microsoft Azure services.\n",
    "    Use the following pieces of context to answer the question at the end. Question is enclosed in <question></question>.\n",
    "    Do keep the following things in mind when answering the question:\n",
    "        - If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "        - Keep the answer as concise as possible.\n",
    "        - Use only the context to answer the question. Context is enclosed in <context></context>\n",
    "        - The context contains one or more paragraph of text that is formatted as markdown. When answering, remove the sentences from the markdown that contain markdown links.\n",
    "        - If the answer is not found in context, simply output \"I'm sorry but I do not know the answer to your question. Please visit Microsoft Learn (https://learn.microsoft.com) or ask a question on StackOverflow (https://stackoverflow.com/questions/tagged/azure).\n",
    "        - Do not include the code in output unless the question is asked to produce the code.\n",
    "        \"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": usr_query},\n",
    "        {\"role\": \"assistant\", \"content\": r_data},\n",
    "    ]\n",
    "\n",
    "    # print(\"\\033[32m----------------------------------------------\\033[0m\")\n",
    "    # print(f\"{messages}\")\n",
    "    # print(\"\\033[32m----------------------------------------------\\033[0m\")\n",
    "\n",
    "    resp = client.chat.completions.create(\n",
    "        model=completions_deployment_name,\n",
    "        messages=messages,\n",
    "        # max_tokens=num_tokens,\n",
    "        temperature=0,\n",
    "        stop=\"+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\\n\",\n",
    "    )\n",
    "\n",
    "    return resp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User_Query: \u001b[32m What are Azure Functions \u001b[0m\n",
      "Response: \u001b[36m Azure Functions is a serverless computing service provided by Microsoft Azure. It allows developers to write and deploy small pieces of code, called functions, that can be triggered by events such as HTTP requests, database changes, or scheduled timers. Azure Functions abstracts away the underlying infrastructure, allowing developers to focus on writing the code that matters most to them. Functions can be written in various programming languages and can integrate with other Azure services to provide feature-rich implementations. \u001b[0m\n",
      "total_tokens: \u001b[36m 1126 \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "matching_docs = get_relevant_docs(usr_query, docs_collection)\n",
    "\n",
    "matching_docs_str = \"\".join(matching_docs[\"documents\"][0])\n",
    "\n",
    "resp = generate_completion(usr_query, matching_docs_str)\n",
    "\n",
    "print(f\"User_Query: \\033[32m {usr_query} \\033[0m\")\n",
    "print(f\"Response: \\033[36m { resp.choices[0].message.content} \\033[0m\")\n",
    "print(f\"total_tokens: \\033[36m { resp.usage.total_tokens} \\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usr_query = input(\"Prompt: \")\n",
    "\n",
    "\n",
    "while usr_query.lower() not in [\"end\", \"quit\", \"exit\", \"stop\"]:\n",
    "    if usr_query.lower() == \"stop\":\n",
    "        break\n",
    "\n",
    "    matching_docs = get_relevant_docs(usr_query, docs_collection)\n",
    "\n",
    "    print(f\"Total docs found: {len(matching_docs['documents'][0])}\")\n",
    "\n",
    "    matching_docs_str = \"\".join(matching_docs[\"documents\"][0])\n",
    "\n",
    "    # print(\"\\033[32m----------------------------------------------\\033[0m\")\n",
    "    # print(f\"{matching_docs}\")\n",
    "    # print(\"\\033[32m----------------------------------------------\\033[0m\")\n",
    "\n",
    "    resp = generate_completion(usr_query, matching_docs_str)\n",
    "\n",
    "    assistant_response = resp.choices[0].message.content\n",
    "\n",
    "    print(f\"total_tokens: \\033[36m { resp.usage.total_tokens} \\033[0m\")\n",
    "    print(f\"User_Query: \\033[32m {usr_query} \\033[0m\")\n",
    "    print(f\"Assistant: \\033[36m { assistant_response} \\033[0m\")\n",
    "\n",
    "    print(\n",
    "        \"\\033[32m\" + \"How can I help you? - Type 'stop' when you are done.\" + \"\\033[0m\"\n",
    "    )\n",
    "\n",
    "    usr_query = input(\"Question: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Compression Techniques\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nltk --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = set(string.punctuation)\n",
    "\n",
    "# Tokenize the sentence\n",
    "word_tokens = word_tokenize(sentence)\n",
    "\n",
    "# Get English stop words\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "\n",
    "def remove_stopwords(text: str) -> str:\n",
    "    tokens = [\n",
    "        token.lower()\n",
    "\n",
    "        for token in text.split()\n",
    "        if token not in stop_words and token not in exclude\n",
    "    ]\n",
    "\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence:The quick brown fox jumps over the very lazy and dog!.\n",
      "Filtered Sentence:\u001b[32mthe quick brown fox jumps lazy dog!.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"The quick brown fox jumps over the very lazy and dog!.\"\n",
    "\n",
    "filtered_text = remove_stopwords(sample_text)\n",
    "\n",
    "# Print the filtered sentence\n",
    "print(\"Original Sentence:\" + sample_text)\n",
    "print(\"Filtered Sentence:\" + \"\\033[32m\" + filtered_text + \"\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User_Query: \u001b[32m What are Azure Functions \u001b[0m\n",
      "Response: \u001b[36m Azure Functions is a serverless solution in Azure that allows you to write less code, maintain less infrastructure, and save on costs. It provides a way to react to critical events and implement various scenarios such as building a web API, responding to database changes, processing event streams or messages, and more. Azure Functions can integrate with a variety of cloud services to provide feature-rich implementations. You can develop Azure Functions using your preferred language and the Azure Functions extension for Visual Studio Code. \u001b[0m\n",
      "total_tokens: \u001b[36m 1746 \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "matching_docs = get_relevant_docs(usr_query, docs_collection)\n",
    "\n",
    "matching_docs_str = \"\".join(matching_docs[\"documents\"][0])\n",
    "\n",
    "filtered_text = remove_stopwords(matching_docs_str)\n",
    "\n",
    "resp = generate_completion(filtered_text, matching_docs_str)\n",
    "\n",
    "print(f\"User_Query: \\033[32m {usr_query} \\033[0m\")\n",
    "print(f\"Response: \\033[36m { resp.choices[0].message.content} \\033[0m\")\n",
    "print(f\"total_tokens: \\033[36m { resp.usage.total_tokens} \\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using LLMLingua\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet llmlingua\n",
    "%pip -qqq install bitsandbytes accelerate\n",
    "\n",
    "# https://github.com/microsoft/LLMLingua/blob/main/examples/RAG.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmlingua import PromptCompressor\n",
    "\n",
    "\n",
    "llm_lingua = PromptCompressor()\n",
    "compressed_prompt = llm_lingua.compress(prompt)\n",
    "\n",
    "\n",
    "\n",
    "# compressed_prompt = llm_lingua.compress_prompt(prompt, instruction=\"\", question=\"\", target_token=200)\n",
    "# Or use the quantation model, like TheBloke/Llama-2-7b-Chat-GPTQ, only need <8GB GPU memory.\n",
    "\n",
    "# Before that, you need to pip install optimum auto-gptq\n",
    "# llm_lingua = PromptCompressor(\"TheBloke/Llama-2-7b-Chat-GPTQ\", model_config={\"revision\": \"main\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up the database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT NEXT LINE TO SKIP THIS CELL EXECUTION\n",
    "# %%script skipping --no-raise-error\n",
    "\n",
    "\n",
    "# To cleanup, you can delete the collection\n",
    "\n",
    "vs_chroma_client.delete_collection()\n",
    "\n",
    "vs_chroma_client.persist()\n",
    "\n",
    "\n",
    "# Or just nuke the persist directory\n",
    "\n",
    "# rm -rf data/processed/dbs/azure_docs/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
